#!/usr/bin/env python
# -*-coding:utf-8 -*-
"""
@File    :		_iterable_dataset.py
@Time    :   	2024/11/21 16:41:03
@Author  :   	Yong Bai 
@Contact :   	baiyong at genomics.cn
@License :   	(C)Copyright 2023-2024, Yong Bai

                Licensed under the Apache License, Version 2.0 (the "License");
                you may not use this file except in compliance with the License.
                You may obtain a copy of the License at

                    http://www.apache.org/licenses/LICENSE-2.0

                Unless required by applicable law or agreed to in writing, software
                distributed under the License is distributed on an "AS IS" BASIS,
                WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
                See the License for the specific language governing permissions and
                limitations under the License.

@Desc    :   	process Huggingface dataset

"""

import logging
from collections import OrderedDict
from functools import partial
import os
from typing import List, Optional, Union
from tqdm import tqdm

from datasets import Dataset, DatasetDict, load_from_disk

from ..utils import (
    check_dir_exists,
    copy_file, 
    write_txt_file,
    down_sampling,
    chunk_sequence,
)

from ..utils._cache import load_from_cache, save_to_cache
from ..utils.constants import (
    SEQENCE_FEATURE_NAME_IN_DATASET_ORIGINAL,
    SEQENCE_FEATURE_NAME_IN_DATASET_CHUNKED
)


logger = logging.getLogger(__name__)

def load_dataset_to_txt(
    input_ds_name: str,
    output_dir: Optional[str]=None,
    output_txt_suffix: Optional[str]=None,
    ds_dict_key: str='train',
    ds_feature_name: str='sequence',
    max_num_examples_per_dataset: int=400000,
    chunk_size: int=20000,
    overlap_step: int=0,
    batch_size: int=5000,
    n_proc: int=16,
    disable_tqdm: bool=False,
):
    """Generate corpus txt file from the input hg dataset

    Parameters
    ----------

    input_ds_name : str,
        HuggingFace dataset names/paths, such as '/path/to/dataset_name'
        The datasets were generated by the script dataset/dataset_gen.py
        - Could be a list of dataset names/paths
    
    ds_dict_key : str,
        key of the dataset to be used, by default 'train'
        - Could be 'train', 'validation', 'test'
        - This is for the case that the input dataset contains multiple splits, 
            that is, the input dataset is a DatasetDict object.
        - It is not used if the input dataset is a Dataset object.
    
    ds_feature_name : str,
        feature name of the dataset to be used, by default 'sequence'
        - This is for the case that the input dataset contains multiple features,
            and the feature contains the sequence data.
             
    output_dir : str, optional
        output directory to save the generated txt file, by default None
        if None, then save the txt file in the input directory

    output_txt_suffix : str
        suffix of the output txt file

    max_num_examples_per_dataset : int, optional
        number of downsampled examples, by default 400000
        - 0: no downsampling
        - This is used for downsampling the training corpus when the corpus is too large.
        - NOTE: when `input_ds_names` is list, the down sampling is applied to each dataset.
          As a result, the real number of examples in the output corpus could be 
        `max_num_examples_per_dataset * len(input_ds_names)`.

    chunk_size : int, optional
        size of each chunk, by default 20000
        - 0: no chunking

    overlap_step : int, optional
        overlap size between two adjacent chunks, by default 200
        - when chunk_size is -1, then overlap_step is not used.

    n_proc : int, optional
        number of processors to be used for parallel processing, by default 16
    
    disable_tqdm : bool, optional
        disable tqdm, by default False

    """

    assert output_dir is not None, "output_dir must be provided."
    check_dir_exists(output_dir, create=True)
    
    output_fname = "corpus.txt" if output_txt_suffix is None else f"corpus_{output_txt_suffix}.txt"
    out_file_name = os.path.join(output_dir, output_fname)

    meta_info = OrderedDict(
        input_ds_name=input_ds_name,
        out_file_name=out_file_name,
        ds_dict_key=ds_dict_key,
        ds_feature_name=ds_feature_name,
        max_num_examples_per_dataset=max_num_examples_per_dataset,
        chunk_size=chunk_size,
        overlap_step=overlap_step,
        batch_size=batch_size,
    )
    
    cache_fname, cache_fsize = load_from_cache(
        out_file_name,
        file_meta = meta_info,
    )

    if cache_fname is not None and cache_fsize > 0:
        copy_file(cache_fname, out_file_name)
        logger.info(f"corpus txt file saved at: {out_file_name}")
        return
    
    _ds = load_dataset_to_dataset(
        input_ds_name,
        ds_dict_key, 
        ds_feature_name,
        max_num_examples=max_num_examples_per_dataset,
        chunk_size=chunk_size,
        overlap_step=overlap_step,
        batch_size=batch_size,
        num_proc=n_proc,
    )

    logger.info(f"writing corpus into file...")
    write_txt_file(out_file_name, _ds['chunked_sequence'], mode='a', n_proc=n_proc, disable_tqdm=disable_tqdm)
    
    cached = save_to_cache(out_file_name, file_meta=meta_info)
    if not cached:
        logger.warning(f"something wrong when caching the file.")

    logger.info(f"corpus txt file saved at: {out_file_name}")


def load_dataset_to_dataset(
    input_corpus_name: str, 
    input_corpus_ds_key: str, 
    input_corpus_ds_feature: str,
    *,
    max_num_examples: int = 0,  # for downsampling
    chunk_size: int = 20000,
    overlap_step: int = 0,
    batch_size: int = 1000,
    num_proc: Optional[int] = 16,
)-> Dataset:
    """ the input HuggingFace datasets, downsampling and chunking the sequence
 

    Parameters
    ----------
    input_corpus_name : str
        input dataset name, such as '/path/to/dataset_directory'
    input_corpus_ds_key : str, 
        when the input dataset is DatasetDict object, 
        then specify the key to be used for training tokenizer, by default 'train'.
        - This is for the case that the input dataset contains multiple splits,
        - e.g., it could be 'train', 'validation', 'test'
    input_corpus_ds_feature : str, optional
        the feature name of the input dataset that saves the sequence, by default 'sequence'
    max_num_examples : int, optional
        the max number of examples to be used, by default 0
        - 0: all examples in the dataset will be used,
        - otherwise, a random subset of examples will be used.
    chunk_size : int, optional
        the size of each chunk for long input sequence, by default 20000
    overlap_step : int, optional
        the overlap size between two adjacent chunks, by default 0
    batch_size : int, optional
        the batch size for `dataset.map`, by default 1000
    num_proc : Optional[int], optional
        the number of processes to be used for chunking the sequence with `dataset.map function`, by default 16

    Returns
    -------
    ds : Dataset
        processed dataset with downsampled, chunked sequence, depending on the input parameters
    """

    assert isinstance(input_corpus_name, str), "paramter input_corpus_name should be a HG dataset path"
    check_dir_exists(input_corpus_name)

    # 0. load the input datasets
    ds = load_from_disk(input_corpus_name)

    if isinstance(ds, DatasetDict):
        ds = ds[input_corpus_ds_key]
    
    # 1. down sampling
    if max_num_examples > 0:
        ds = down_sampling(ds, max_num_examples)

    # 2. chunking the sequence
    # TODO: test when batch_size > BATCH_NUM_SEQS
    if chunk_size > 0:
        chunk_func = partial(chunk_sequence, chunk_size=chunk_size, overlap_step=overlap_step, n_proc=num_proc)
        ds = ds.map(
            lambda examples: {
                SEQENCE_FEATURE_NAME_IN_DATASET_CHUNKED: chunk_func(examples[input_corpus_ds_feature])
            }, 
            batched=True,
            batch_size=batch_size,
            num_proc=num_proc, 
            remove_columns=ds.column_names
        )
    
    return ds, 



