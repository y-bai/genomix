{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from genomix.utils import SPECIAL_TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d467c4388a6f46eaa284bbd46bb5c33a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sequence', 'chr'],\n",
       "        num_rows: 3\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sequence', 'chr'],\n",
       "        num_rows: 3\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequence = ['CACCCTAAACCCTAACCCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCCTAAACCCT', 'ACCCTCACCCTCACCCTCACCCTCACCCTCACCCTCACCCTCACCCTAACCCTAACCCTAACCC']\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "def gen_demo_data():\n",
    "    yield {\"sequence\": \"GACCCTAAACCCTAACCCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCCTAAACCCT\", \"chr\": \"1\"}\n",
    "    yield {\"sequence\": \"ACCCTCACCCTCACCCTCACCCTCACCCTCACCCTCACCCTCACCCTAACCCTAACCCTAACCC\", \"chr\": \"1\"}\n",
    "    yield {\"sequence\": \"ACCCTCACCCTCAGGCTCACCCTCACCCTCACCCTCACCCTCACCCTAACCCTAACCCTAACCC\", \"chr\": \"1\"}\n",
    "\n",
    "ds1 = Dataset.from_generator(gen_demo_data)\n",
    "ds2 = Dataset.from_generator(gen_demo_data)\n",
    "\n",
    "\n",
    "ds = DatasetDict()\n",
    "ds[\"train\"] = ds1\n",
    "ds[\"validation\"] = ds2\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GACCCTAAACCCTAACCCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCCTAAACCCT',\n",
       " 'ACCCTCACCCTCACCCTCACCCTCACCCTCACCCTCACCCTCACCCTAACCCTAACCCTAACCC']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "ds[\"train\"][np.array([0,1])]['sequence']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BioSeqBPETokenizer\n",
    "\n",
    "slow version of BPE tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<jemalloc>: Unsupported system page size\n"
     ]
    }
   ],
   "source": [
    "from genomix.tokenizers import BioSeqBPETokenizer\n",
    "\n",
    "tokenizer_kwargs = {\n",
    "        \"bos_token\": SPECIAL_TOKENS.BOS.value,\n",
    "        \"eos_token\": SPECIAL_TOKENS.EOS.value,\n",
    "        \"unk_token\": SPECIAL_TOKENS.UNK.value,\n",
    "        \"mask_token\": SPECIAL_TOKENS.MASK.value,\n",
    "        \"padding_side\": \"left\", # as a prediction next token model, the padding is done on the left\n",
    "        \"add_bos_token\": False,\n",
    "        \"add_eos_token\": False,\n",
    "        \"add_prefix_space\": False, \n",
    "        \"do_lower_case\": False,\n",
    "        # \"model_max_length\": 6  # This should be set when do tokenization, not in __init__\n",
    "    }\n",
    "\n",
    "# we use the shrinked vocabulary for the tokenizer for test\n",
    "vocab_dir = '/home/share/huadjyin/home/baiyong01/projects/genomix/tmp'\n",
    "tokenier = BioSeqBPETokenizer.from_pretrained(\n",
    "    vocab_dir, \n",
    "    local_files_only=True, \n",
    "    **tokenizer_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenier.vocab_size\n",
    "## You can add tokens to the vocabulary by providing a list of strings \n",
    "# tokenier.add_tokens(['ADDED'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Because the following tokenization is set with `padding = \"max_length\"`\n",
    "# we have to set `pad_token` explicitly, otherwise the errror will be raised:\n",
    "# *************************************************************************\n",
    "# ValueError: Asking to pad but the tokenizer does not have a padding token. \n",
    "# Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \n",
    "# or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`. \n",
    "\n",
    "tokenier.pad_token = SPECIAL_TOKENS.EOS.value\n",
    "\n",
    "tokenier.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizations = tokenier(input_sequence[0], \n",
    "                        #  add_special_tokens=True,\n",
    "                         max_length=6, # we set the max_length to 6 here, so the output will be truncated\n",
    "                         truncation = True,\n",
    "                         padding = \"max_length\",\n",
    "                         stride=2,\n",
    "                         return_overflowing_tokens = True\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'overflowing_tokens': [23, 5, 21, 13, 13, 21, 37, 21, 37, 21, 37, 21, 37, 21, 37, 21, 13, 13, 21, 23, 5, 8], 'num_truncated_tokens': 20, 'input_ids': [5, 23, 5, 21, 23, 5], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: from the output above, we can see that the output returns ONLY ONE input_ids.\n",
    "\n",
    "This is because we use the SLOW version of tokenizer, and thus `return_overflowing_tokens` DO NOT affect the tokenizer.\n",
    "\n",
    "see https://github.com/huggingface/transformers/issues/23001 for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'overflowing_tokens': [23, 5, 21, 13, 13, 21, 37, 21, 37, 21, 37, 21, 37, 21, 37, 21, 13, 13, 21, 23, 5, 8], 'num_truncated_tokens': 20, 'input_ids': [5, 23, 5, 21, 23, 5], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we test the add_special_tokens=True\n",
    "tokenizations = tokenier(input_sequence[0], \n",
    "                         add_special_tokens=True,\n",
    "                         max_length=6, # we set the max_length to 6 here, so the output will be truncated\n",
    "                         truncation = True,\n",
    "                         padding = \"max_length\",\n",
    "                         stride=2,\n",
    "                         return_overflowing_tokens = True\n",
    "                         )\n",
    "tokenizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the output is the same as the results above, no matter if we set ` add_special_tokens=True` or not. \n",
    "\n",
    "This is becase we iniialized the tokenizer with `\"add_bos_token\": False` and `\"add_eos_token\": False`.\n",
    "\n",
    "**NOTE**\n",
    "if `\"add_bos_token\": True` and `\"add_eos_token\": Ture`, then:\n",
    "* if ` add_special_tokens=True` (* Default value by `transformers` *), then the output will add `BOS` and `EOS`; \n",
    "* if ` add_special_tokens=False`, then the output will NOT add `BOS` and `EOS`; \n",
    "\n",
    "\n",
    "```python\n",
    "tokenizer_kwargs = {\n",
    "        \"bos_token\": SPECIAL_TOKENS.BOS.value,\n",
    "        \"eos_token\": SPECIAL_TOKENS.EOS.value,\n",
    "        \"unk_token\": SPECIAL_TOKENS.UNK.value,\n",
    "        \"mask_token\": SPECIAL_TOKENS.MASK.value,\n",
    "        \"padding_side\": \"left\", # as a prediction next token model, the padding is done on the left\n",
    "        \"add_bos_token\": False,\n",
    "        \"add_eos_token\": False,\n",
    "        \"add_prefix_space\": False, \n",
    "        \"do_lower_case\": False,\n",
    "        # \"model_max_length\": 6  # This should be set when do tokenization, not in __init__\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we re-initialize the tokenizer with `add_bos_token=True` and `add_eos_token=True`\n",
    "\n",
    "tokenizer_kwargs = {\n",
    "        \"bos_token\": SPECIAL_TOKENS.BOS.value,\n",
    "        \"eos_token\": SPECIAL_TOKENS.EOS.value,\n",
    "        \"unk_token\": SPECIAL_TOKENS.UNK.value,\n",
    "        \"mask_token\": SPECIAL_TOKENS.MASK.value,\n",
    "        \"padding_side\": \"left\", # as a prediction next token model, the padding is done on the left\n",
    "        \"add_bos_token\": True,\n",
    "        \"add_eos_token\": True,\n",
    "        \"add_prefix_space\": False, \n",
    "        \"do_lower_case\": False,\n",
    "        # \"model_max_length\": 6  # This should be set when do tokenization, not in __init__\n",
    "    }\n",
    "\n",
    "# we use the shrinked vocabulary for the tokenizer for test\n",
    "vocab_dir = '/home/share/huadjyin/home/baiyong01/projects/genomix/tmp'\n",
    "tokenier = BioSeqBPETokenizer.from_pretrained(\n",
    "    vocab_dir, \n",
    "    local_files_only=True, \n",
    "    **tokenizer_kwargs)\n",
    "\n",
    "# add `PAD` token\n",
    "tokenier.pad_token = SPECIAL_TOKENS.EOS.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'overflowing_tokens': [23, 5, 21, 13, 13, 21, 37, 21, 37, 21, 37, 21, 37, 21, 37, 21, 13, 13, 21, 23, 5, 8], 'num_truncated_tokens': 20, 'input_ids': [5, 23, 5, 21, 23, 5], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we test the add_special_tokens=False\n",
    "tokenizations = tokenier(input_sequence[0], \n",
    "                         add_special_tokens=False,\n",
    "                         max_length=6, # we set the max_length to 6 here, so the output will be truncated\n",
    "                         truncation = True,\n",
    "                         padding = \"max_length\",\n",
    "                         stride=2,\n",
    "                         return_overflowing_tokens = True\n",
    "                         )\n",
    "tokenizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'overflowing_tokens': [5, 21, 23, 5, 21, 13, 13, 21, 37, 21, 37, 21, 37, 21, 37, 21, 37, 21, 13, 13, 21, 23, 5, 8], 'num_truncated_tokens': 22, 'input_ids': [0, 5, 23, 5, 21, 2], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we test the add_special_tokens=True\n",
    "tokenizations = tokenier(input_sequence[0], \n",
    "                         add_special_tokens=True,\n",
    "                         max_length=6, # we set the max_length to 6 here, so the output will be truncated\n",
    "                         truncation = True,\n",
    "                         padding = \"max_length\",\n",
    "                         stride=2,\n",
    "                         return_overflowing_tokens = True\n",
    "                         )\n",
    "tokenizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have special tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BioSeqBPETokenizerFast\n",
    "\n",
    "fast version of BPE tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genomix.tokenizers import BioSeqBPETokenizerFast\n",
    "\n",
    "tokenizer_kwargs = {\n",
    "        \"bos_token\": SPECIAL_TOKENS.BOS.value,\n",
    "        \"eos_token\": SPECIAL_TOKENS.EOS.value,\n",
    "        \"unk_token\": SPECIAL_TOKENS.UNK.value,\n",
    "        \"mask_token\": SPECIAL_TOKENS.MASK.value,\n",
    "        \"padding_side\": \"left\", # as a prediction next token model, the padding is done on the left\n",
    "        \"add_bos_token\": True,\n",
    "        \"add_eos_token\": True,\n",
    "        \"add_prefix_space\": False, \n",
    "        \"do_lower_case\": False,\n",
    "        # \"model_max_length\": 6  # This should be set when do tokenization, not in __init__\n",
    "    }\n",
    "\n",
    "# we use the shrinked vocabulary for the tokenizer for test\n",
    "vocab_dir = '/home/share/huadjyin/home/baiyong01/projects/genomix/tmp'\n",
    "tokenier = BioSeqBPETokenizerFast.from_pretrained(\n",
    "    vocab_dir, \n",
    "    local_files_only=True, \n",
    "    **tokenizer_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add `PAD` token\n",
    "tokenier.pad_token = SPECIAL_TOKENS.EOS.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 5, 23, 5, 21, 2], [0, 5, 21, 23, 5, 2], [0, 23, 5, 21, 13, 2], [0, 21, 13, 13, 21, 2], [0, 13, 21, 37, 21, 2], [0, 37, 21, 37, 21, 2], [0, 37, 21, 37, 21, 2], [0, 37, 21, 37, 21, 2], [0, 37, 21, 37, 21, 2], [0, 37, 21, 13, 13, 2], [0, 13, 13, 21, 23, 2], [0, 21, 23, 5, 8, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]], 'overflow_to_sample_mapping': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizations = tokenier(input_sequence[0], \n",
    "                         add_special_tokens=True,\n",
    "                         max_length=6, # we set the max_length to 6 here, so the output will be truncated\n",
    "                         truncation = True,\n",
    "                         padding = \"max_length\",\n",
    "                         stride=2,\n",
    "                         return_overflowing_tokens = True\n",
    "                         )\n",
    "tokenizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[5, 23, 5, 21, 23, 5], [23, 5, 21, 13, 13, 21], [13, 21, 37, 21, 37, 21], [37, 21, 37, 21, 37, 21], [37, 21, 37, 21, 13, 13], [13, 13, 21, 23, 5, 8]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]], 'overflow_to_sample_mapping': [0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizations = tokenier(input_sequence[0], \n",
    "                         add_special_tokens=False,\n",
    "                         max_length=6, # we set the max_length to 6 here, so the output will be truncated\n",
    "                         truncation = True,\n",
    "                         padding = \"max_length\",\n",
    "                         stride=2,\n",
    "                         return_overflowing_tokens = True\n",
    "                         )\n",
    "tokenizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets test\n",
    "\n",
    "**NOTE**\n",
    "\n",
    "when using pyarrow version 16.1.0, the error will be raised:\n",
    "\n",
    "`<jemalloc>`: Unsupported system page size\n",
    "\n",
    "see: https://github.com/apache/arrow/issues/11134\n",
    "\n",
    "This will lead the error when create datatset:\n",
    "\n",
    "```python\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "def gen_demo_data():\n",
    "    yield {\"sequence\": \"GACCCTAAACCCTAACCCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCCTAAACCCT\", \"char\": \"1\"}\n",
    "    yield {\"sequence\": \"ACCCTCACCCTCACCCTCACCCTCACCCTCACCCTCACCCTCACCCTAACCCTAACCCTAACCC\", \"chr\": \"1\"}\n",
    "\n",
    "ds1 = Dataset.from_generator(gen_demo_data)\n",
    "\n",
    "```\n",
    "\n",
    "ArrowMemoryError: malloc of size 256 failed.\n",
    "\n",
    "OSError: [Errno 39] Directory not empty: '~/.cache/huggingface/datasets/generator/default-1d7aac6694688fc4/0.0.0.incomplete'\n",
    "\n",
    "*************************************************************************\n",
    "\n",
    "**SOLVED**: Upgrade the pyarrow to 16.1.0 to version 18.0.0 solve the problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e95be3ed6cd1471e8d2b557e9a252f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sequence', 'chr'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sequence', 'chr'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "def gen_demo_data():\n",
    "    yield {\"sequence\": \"GACCCTAAACCCTAACCCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCCTAAACCCT\", \"chr\": \"1\"}\n",
    "    yield {\"sequence\": \"ACCCTCACCCTCACCCTCACCCTCACCCTCACCCTCACCCTCACCCTAACCCTAACCCTAACCC\", \"chr\": \"1\"}\n",
    "\n",
    "ds1 = Dataset.from_generator(gen_demo_data)\n",
    "ds2 = Dataset.from_generator(gen_demo_data)\n",
    "\n",
    "ds = DatasetDict()\n",
    "ds[\"train\"] = ds1\n",
    "ds[\"validation\"] = ds2\n",
    "\n",
    "ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genomix.tokenizers import BioSeqBPETokenizerFast\n",
    "\n",
    "tokenizer_kwargs = {\n",
    "        \"bos_token\": SPECIAL_TOKENS.BOS.value,\n",
    "        \"eos_token\": SPECIAL_TOKENS.EOS.value,\n",
    "        \"unk_token\": SPECIAL_TOKENS.UNK.value,\n",
    "        \"mask_token\": SPECIAL_TOKENS.MASK.value,\n",
    "        \"padding_side\": \"left\", # as a prediction next token model, the padding is done on the left\n",
    "        \"add_bos_token\": True,\n",
    "        \"add_eos_token\": False,\n",
    "        \"add_prefix_space\": False, \n",
    "        \"do_lower_case\": False,\n",
    "        # \"model_max_length\": 6  # This should be set when do tokenization, not in __init__\n",
    "    }\n",
    "\n",
    "# we use the shrinked vocabulary for the tokenizer for test\n",
    "vocab_dir = '/home/share/huadjyin/home/baiyong01/projects/genomix/tmp'\n",
    "tokenier = BioSeqBPETokenizerFast.from_pretrained(\n",
    "    vocab_dir, \n",
    "    local_files_only=True, \n",
    "    **tokenizer_kwargs)\n",
    "\n",
    "# add `PAD` token\n",
    "tokenier.pad_token = SPECIAL_TOKENS.EOS.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 6, 23, 5, 21, 23], [0, 21, 23, 5, 21, 13], [0, 21, 13, 13, 21, 37], [0, 21, 37, 21, 37, 21], [0, 37, 21, 37, 21, 37], [0, 21, 37, 21, 37, 21], [0, 37, 21, 13, 13, 21], [0, 13, 21, 23, 5, 8]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]], 'overflow_to_sample_mapping': [0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds = tokenier(ds['train']['sequence'][0], \n",
    "                        max_length=6, \n",
    "                        truncation=True,\n",
    "                        padding=True,\n",
    "                        return_overflowing_tokens=True, \n",
    "                        stride=2, \n",
    "                        add_special_tokens=True)\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genomix.tokenizers import BioSeqUnigramTokenizerFast\n",
    "\n",
    "tokenizer_kwargs = {\n",
    "        \"bos_token\": SPECIAL_TOKENS.BOS.value,\n",
    "        \"eos_token\": SPECIAL_TOKENS.EOS.value,\n",
    "        \"unk_token\": SPECIAL_TOKENS.UNK.value,\n",
    "        \"mask_token\": SPECIAL_TOKENS.MASK.value,\n",
    "        \"padding_side\": \"left\", # as a prediction next token model, the padding is done on the left\n",
    "        \"add_bos_token\": True,\n",
    "        \"add_eos_token\": True,\n",
    "        \"add_prefix_space\": False, \n",
    "        \"do_lower_case\": False,\n",
    "        # \"model_max_length\": 6  # This should be set when do tokenization, not in __init__\n",
    "    }\n",
    "\n",
    "# we use the shrinked vocabulary for the tokenizer for test\n",
    "vocab_dir = '/home/share/huadjyin/home/baiyong01/projects/genomix/tmp'\n",
    "tokenier = BioSeqUnigramTokenizerFast.from_pretrained(\n",
    "    vocab_dir, \n",
    "    local_files_only=True, \n",
    "    # **tokenizer_kwargs  # test the default setting\n",
    "    )\n",
    "\n",
    "# add `PAD` token\n",
    "tokenier.pad_token = SPECIAL_TOKENS.EOS.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<BOS>', '<EOS>', '<UNK>', None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenier.bos_token, tokenier.eos_token, tokenier.unk_token, tokenier.mask_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above shows that the default setting leads `tokenier.mask_token` to `None` even the `unigram.json` contains `<MASK>` token.\n",
    "\n",
    "Therefore, we have to set the tokenier parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_kwargs = {\n",
    "        \"bos_token\": SPECIAL_TOKENS.BOS.value,\n",
    "        \"eos_token\": SPECIAL_TOKENS.EOS.value,\n",
    "        \"unk_token\": SPECIAL_TOKENS.UNK.value,\n",
    "        \"mask_token\": SPECIAL_TOKENS.MASK.value,\n",
    "        \"padding_side\": \"left\", # as a prediction next token model, the padding is done on the left\n",
    "        \"add_bos_token\": True,\n",
    "        \"add_eos_token\": True,\n",
    "        \"add_prefix_space\": False, \n",
    "        \"do_lower_case\": False,\n",
    "        # \"model_max_length\": 6  # This should be set when do tokenization, not in __init__\n",
    "    }\n",
    "\n",
    "# we use the shrinked vocabulary for the tokenizer for test\n",
    "vocab_dir = '/home/share/huadjyin/home/baiyong01/projects/genomix/tmp'\n",
    "tokenier = BioSeqUnigramTokenizerFast.from_pretrained(\n",
    "    vocab_dir, \n",
    "    local_files_only=True, \n",
    "    **tokenizer_kwargs  # test the default setting\n",
    "    )\n",
    "\n",
    "# add `PAD` token\n",
    "tokenier.pad_token = SPECIAL_TOKENS.EOS.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<BOS>', '<EOS>', '<UNK>', '<MASK>')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenier.bos_token, tokenier.eos_token, tokenier.unk_token, tokenier.mask_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPM tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: <sentencepiece.SentencePieceProcessor; proxy of <Swig Object of type 'sentencepiece::SentencePieceProcessor *' at 0x4004b4967030> >\n"
     ]
    }
   ],
   "source": [
    "from genomix.tokenizers import BioSeqSPMTokenizerFast, BioSeqSPMTokenizer\n",
    "\n",
    "tokenizer_kwargs = {\n",
    "        \"bos_token\": SPECIAL_TOKENS.BOS.value,\n",
    "        \"eos_token\": SPECIAL_TOKENS.EOS.value,\n",
    "        \"unk_token\": SPECIAL_TOKENS.UNK.value,\n",
    "        \"mask_token\": SPECIAL_TOKENS.MASK.value,\n",
    "        \"padding_side\": \"left\", # as a prediction next token model, the padding is done on the left\n",
    "        \"add_bos_token\": True,\n",
    "        \"add_eos_token\": True,\n",
    "        \"add_prefix_space\": False, \n",
    "        \"do_lower_case\": False,\n",
    "        # \"model_max_length\": 6  # This should be set when do tokenization, not in __init__\n",
    "    }\n",
    "\n",
    "# we use the shrinked vocabulary for the tokenizer for test\n",
    "vocab_dir = '/home/share/huadjyin/home/baiyong01/projects/genomix/tmp'\n",
    "tokenier = BioSeqSPMTokenizer.from_pretrained(\n",
    "    vocab_dir, \n",
    "    local_files_only=True, \n",
    "    **tokenizer_kwargs  # test the default setting\n",
    "    )\n",
    "\n",
    "# add `PAD` token\n",
    "tokenier.pad_token = SPECIAL_TOKENS.EOS.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<BOS>', '<EOS>', '<UNK>', '<MASK>', '<EOS>')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenier.bos_token, tokenier.eos_token, tokenier.unk_token, tokenier.mask_token, tokenier.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2, 1, 53, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenier.bos_token_id, tokenier.eos_token_id, tokenier.unk_token_id, tokenier.mask_token_id, tokenier.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: <sentencepiece.SentencePieceProcessor; proxy of <Swig Object of type 'sentencepiece::SentencePieceProcessor *' at 0x4002a3e1d440> >\n"
     ]
    }
   ],
   "source": [
    "from genomix.tokenizers import BioSeqSPMTokenizerFast, BioSeqSPMTokenizer\n",
    "\n",
    "tokenizer_kwargs = {\n",
    "        \"bos_token\": SPECIAL_TOKENS.BOS.value,\n",
    "        \"eos_token\": SPECIAL_TOKENS.EOS.value,\n",
    "        \"unk_token\": SPECIAL_TOKENS.UNK.value,\n",
    "        \"mask_token\": SPECIAL_TOKENS.MASK.value,\n",
    "        \"padding_side\": \"left\", # as a prediction next token model, the padding is done on the left\n",
    "        \"add_bos_token\": True,\n",
    "        \"add_eos_token\": True,\n",
    "        \"add_prefix_space\": False, \n",
    "        \"do_lower_case\": False,\n",
    "        # \"model_max_length\": 6  # This should be set when do tokenization, not in __init__\n",
    "    }\n",
    "\n",
    "# we use the shrinked vocabulary for the tokenizer for test\n",
    "vocab_dir = '/home/share/huadjyin/home/baiyong01/projects/genomix/tmp'\n",
    "tokenier = BioSeqSPMTokenizerFast.from_pretrained(\n",
    "    vocab_dir, \n",
    "    local_files_only=True, \n",
    "    **tokenizer_kwargs  # test the default setting\n",
    "    )\n",
    "\n",
    "# add `PAD` token\n",
    "tokenier.pad_token = SPECIAL_TOKENS.EOS.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'overflowing_tokens': [17, 3, 38, 17, 16, 16, 14, 3, 16, 14, 3, 16, 14, 3, 16, 14, 3, 16, 14, 3, 38, 17, 16, 38, 17, 3, 38], 'num_truncated_tokens': 25, 'input_ids': [0, 47, 38, 17, 3, 2], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizations = tokenier(input_sequence[0], \n",
    "                         add_special_tokens=True,\n",
    "                         max_length=6, # we set the max_length to 6 here, so the output will be truncated\n",
    "                         truncation = True,\n",
    "                         padding = \"max_length\",\n",
    "                         stride=2,\n",
    "                         return_overflowing_tokens = True\n",
    "                         )\n",
    "tokenizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<BOS> CAC CCT AA AC <EOS>'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenier.decode(tokenizations['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
