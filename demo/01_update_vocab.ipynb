{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update vocab\n",
    "\n",
    "This is for testing the vocab update functionality.\n",
    "\n",
    "See `utils.update_vocab` for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "from genomix.utils import update_vocab, INITIAL_ALPHABETS, SPECIAL_TOKENS, TOKENIZER_MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/home/share/huadjyin/home/baiyong01/projects/genomix/tmp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update BPE vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input directory: /home/share/huadjyin/home/baiyong01/projects/genomix/tokens/20000_200/T2T/BPE/200008\n",
      "Output directory: /home/share/huadjyin/home/baiyong01/projects/genomix/tmp\n",
      "Tokenizer model: BPE\n",
      "Vocabulary file name: ['vocab.json', 'merges.txt']\n",
      "New special tokens: ['<BOS>', '<UNK>', '<EOS>', '<MASK>']\n",
      "New vocabulary size: 59\n",
      "--------------------\n",
      "Start updating the vocabulary file for BPE tokenizer.\n",
      "DONE. Updated vocab file is saved to /home/share/huadjyin/home/baiyong01/projects/genomix/tmp for BPE tokenizer.\n"
     ]
    }
   ],
   "source": [
    "input_dir = '/home/share/huadjyin/home/baiyong01/projects/genomix/tokens/20000_200/T2T/BPE/200008'\n",
    "\n",
    "tokenizer_model = TOKENIZER_MODELS.BPE.value\n",
    "vocab_fname = [\"vocab.json\", \"merges.txt\"]\n",
    "new_special_tokens = SPECIAL_TOKENS.values() \n",
    "new_vocab_size = 50 + len(INITIAL_ALPHABETS) + len(new_special_tokens)\n",
    "\n",
    "update_vocab(\n",
    "    input_dir, \n",
    "    output_dir, \n",
    "    tokenizer_model=tokenizer_model, \n",
    "    vocab_fname=vocab_fname, \n",
    "    new_special_tokens=new_special_tokens, \n",
    "    new_vocab_size=new_vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Unigram vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input directory: /home/share/huadjyin/home/baiyong01/projects/biomlm/biomlm/mambaDNA/tokens/T2T/1K/Unigram/200010\n",
      "Output directory: /home/share/huadjyin/home/baiyong01/projects/genomix/tmp\n",
      "Tokenizer model: UNIGRAM\n",
      "Vocabulary file name: unigram.json\n",
      "New special tokens: ['<BOS>', '<UNK>', '<EOS>', '<MASK>']\n",
      "New vocabulary size: 54\n",
      "--------------------\n",
      "Start updating the vocabulary file for UNIGRAM tokenizer.\n",
      "DONE. Updated vocab file is saved to /home/share/huadjyin/home/baiyong01/projects/genomix/tmp for UNIGRAM tokenizer.\n"
     ]
    }
   ],
   "source": [
    "input_dir = '/home/share/huadjyin/home/baiyong01/projects/biomlm/biomlm/mambaDNA/tokens/T2T/1K/Unigram/200010'\n",
    "\n",
    "tokenizer_model = TOKENIZER_MODELS.UNIGRAM.value\n",
    "vocab_fname = \"unigram.json\"\n",
    "new_special_tokens = SPECIAL_TOKENS.values() \n",
    "new_vocab_size = 50 + len(new_special_tokens) # Unigram not need initial alphabets\n",
    "\n",
    "update_vocab(\n",
    "    input_dir, \n",
    "    output_dir, \n",
    "    tokenizer_model=tokenizer_model, \n",
    "    vocab_fname=vocab_fname, \n",
    "    new_special_tokens=new_special_tokens, \n",
    "    new_vocab_size=new_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update SPM vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input directory: /home/share/huadjyin/home/baiyong01/projects/genomix/tokens/20000_200/T2T/SPM_Unigram/200008\n",
      "Output directory: /home/share/huadjyin/home/baiyong01/projects/genomix/tmp\n",
      "Tokenizer model: SPM\n",
      "Vocabulary file name: ['spm_vocab.model', 'spm_vocab.vocab']\n",
      "New special tokens: ['<BOS>', '<EOS>', '<UNK>', '<MASK>']\n",
      "New vocabulary size: 54\n",
      "--------------------\n",
      "Start updating the vocabulary file for SPM tokenizer.\n",
      "Updating/removing 199955 tokens from the end of the vocabulary file...\n",
      "DONE. Updated vocab file is saved to /home/share/huadjyin/home/baiyong01/projects/genomix/tmp for SPM tokenizer.\n"
     ]
    }
   ],
   "source": [
    "input_dir = '/home/share/huadjyin/home/baiyong01/projects/genomix/tokens/20000_200/T2T/SPM_Unigram/200008'\n",
    "\n",
    "tokenizer_model = TOKENIZER_MODELS.SPM.value\n",
    "vocab_fname = ['spm_vocab.model', 'spm_vocab.vocab']\n",
    "new_special_tokens = SPECIAL_TOKENS.values()\n",
    "new_vocab_size = 50 + len(new_special_tokens) # SPM not need initial alphabets\n",
    "\n",
    "update_vocab(\n",
    "    input_dir, \n",
    "    output_dir, \n",
    "    tokenizer_model=tokenizer_model, \n",
    "    vocab_fname=vocab_fname, \n",
    "    new_special_tokens=new_special_tokens, \n",
    "    new_vocab_size=new_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
