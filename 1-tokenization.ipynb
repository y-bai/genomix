{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo and test tokenizatoin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE tokenization\n",
    "\n",
    "Suppose we have train the BPE tokenizer using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*args: ('input_dir_path', 'output_dir_path')\n",
      "Updating vocab for tokenizer type: BPE\n",
      "BPE vocab update called\n",
      "*args: ('input_dir_path', 'output_dir_path')\n",
      "Updating vocab for tokenizer type: UNIGRAM\n",
      "Unigram vocab update called\n",
      "*args: ('input_dir_path', 'output_dir_path')\n",
      "Updating vocab for tokenizer type: SPM\n",
      "SPM vocab update called\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Tuple, Optional, Union\n",
    "from functools import wraps\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def write_json(filepath, data):\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "def _bpe_vocab_update(*args, **kwargs):\n",
    "    print(\"BPE vocab update called\")\n",
    "    # Actual implementation here\n",
    "\n",
    "def _unigram_vocab_update(*args, **kwargs):\n",
    "    print(\"Unigram vocab update called\")\n",
    "    # Actual implementation here\n",
    "\n",
    "def _spm_vocab_update(*args, **kwargs):\n",
    "    print(\"SPM vocab update called\")\n",
    "    # Actual implementation here\n",
    "\n",
    "# Define the decorator\n",
    "def vocab_update(func):\n",
    "\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # Print or log the *args value\n",
    "        print(f\"*args: {args}\")\n",
    "        logger.info(f\"*args: {args}\")\n",
    "\n",
    "        # Call the original update_vocab function\n",
    "        func(*args, **kwargs)\n",
    "        \n",
    "        # Then call the appropriate vocab update function\n",
    "        tokenizer_model = kwargs.get('tokenizer_model')\n",
    "        \n",
    "        if tokenizer_model == 'BPE':\n",
    "            return _bpe_vocab_update(*args, **kwargs)\n",
    "        elif tokenizer_model == 'UNIGRAM':\n",
    "            return _unigram_vocab_update(*args, **kwargs)\n",
    "        elif tokenizer_model == 'SPM':\n",
    "            return _spm_vocab_update(*args, **kwargs)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown tokenizer model: {tokenizer_model}\")\n",
    "    \n",
    "    return wrapper\n",
    "\n",
    "@vocab_update\n",
    "def update_vocab(\n",
    "        input_dir: str,\n",
    "        output_dir: str,\n",
    "        tokenizer_model: str=\"BPE\",\n",
    "        tokenizer_spm_sublevel: Optional[str]=\"UNIGRAM\",\n",
    "        vocab_fname: Union[List[str], Tuple[str, ...]]=[\"vocab.json\", \"merges.txt\"],\n",
    "        new_special_tokens: Optional[List[str]]=[\"<BOS>\", \"<UNK>\", \"<EOS>\", \"<MASK>\"],\n",
    "        new_vocab_size: int=5009,\n",
    "):\n",
    "    \"\"\"\n",
    "    Update the vocabulary file for the model.\n",
    "\n",
    "    Args:\n",
    "    - input_dir: str, the model name or path\n",
    "        the dictionary name that contains the vocabulary files\n",
    "\n",
    "    - output_dir: str, the output directory\n",
    "\n",
    "    - tokenizer_model: str, the tokenizer type, default is \"BPE\"\n",
    "        Could be \"BPE\" | \"UNIGRAM\" | \"SPM\"\n",
    "        NOTE: For \"SPM\", we only consider the SPM-unigram tokenizer for now.\n",
    "\n",
    "    - tokenizer_spm_sublevel: str, the sub-level of SPM tokenizer, default is \"UNIGRAM\"\n",
    "        Could be \"UNIGRAM\" | \"BPE\". \n",
    "        NOTE: This parameter is only used for SPM tokenizer.\n",
    "\n",
    "    - vocab_fname: List[str], the vocabulary file name, default is [\"vocab.json\", \"merges.txt\"]\n",
    "        For BPE tokenizer, the vocabulary file is \"vocab.json\" and \"merges.txt\"\n",
    "        For UNIGRAM tokenizer, the vocabulary file is \"unigram.json\"\n",
    "        For SPM tokenizer, the vocabulary file is \"spm_vocab.model\" and \"spm_vocab.vocab\"\n",
    "    \"\"\"\n",
    "    tokenizer_type =tokenizer_model\n",
    "    print(f\"Updating vocab for tokenizer type: {tokenizer_type}\")\n",
    "    # Additional logic can be added here if needed\n",
    "\n",
    "# Example calls\n",
    "update_vocab(\"input_dir_path\", \"output_dir_path\", tokenizer_model='BPE')\n",
    "update_vocab(\"input_dir_path\", \"output_dir_path\", tokenizer_model='UNIGRAM')\n",
    "update_vocab(\"input_dir_path\", \"output_dir_path\", tokenizer_model='SPM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
